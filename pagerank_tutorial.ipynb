{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook shows how fundamental concepts of pagerank can be translated into code. I use 'Mining of Massive Datasets' book by Jure Leskovec, Anand Rajaraman and Jeff Ullman as a reference.\n",
    "\n",
    "Link to the book: http://www.mmds.org/\n",
    "\n",
    "I assume that reader has a basic understanding of Pagerank. If not, you may want to look at the related chapter of 'Mining of Massive Datasets' book.\n",
    "\n",
    "## Coverage\n",
    "\n",
    "0. Introduction\n",
    "  - 0.1. Web/Graph Representation\n",
    "1. Simplified PageRank\n",
    "2. Dead-ends and Spider Traps\n",
    "3. PageRank with Random Teleportation (Taxation)\n",
    "4. Spam-Farms\n",
    "5. TrustRank\n",
    "6. Apply PageRank on a Real Dataset\n",
    "7. One More Optimization\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "### 0.1. Web/Graph Representation\n",
    "\n",
    "Given a directed graph, we'll represent it as dictionary of lists. Each node of the graph will be a key of the dictionary (`G`) and outgoing edges of a particular node `u` will be a list pointed by `G[u]`. Note that, even though a node may not have any outgoing links we still assign a value (empty list) to that key in the dictionary.\n",
    "\n",
    "Figure - Toy Graph:\n",
    "![1.png](1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_graph():\n",
    "    G = dict()\n",
    "    G[0] = [1, 2, 3]\n",
    "    G[1] = [0, 3]\n",
    "    G[2] = [0]\n",
    "    G[3] = [1, 2]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simplified Pagerank \n",
    "\n",
    "Pagerank problem is simply finding stationary distribution of states in a markov model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, iteration_count=100):\n",
    "    \n",
    "    N = len(G.keys())\n",
    "    next_rank_lst = [1/N for _ in range(N)]\n",
    "    current_rank_lst = next_rank_lst[:]\n",
    "    \n",
    "    for i in range(iteration_count):\n",
    "        current_rank_lst, next_rank_lst = next_rank_lst, current_rank_lst\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] = 0\n",
    "        for node in G:\n",
    "            if G[node]: # To avoid division by zero problem in the next line\n",
    "                contribution = current_rank_lst[node] / len(G[node])\n",
    "                for edge in G[node]:\n",
    "                    next_rank_lst[edge] += contribution\n",
    "    \n",
    "    return next_rank_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.2222222222222222, 0.2222222222222222, 0.2222222222222222]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "G = toy_graph()\n",
    "rank_lst = pagerank(G)\n",
    "print(rank_lst)\n",
    "print(sum(rank_lst)) # Total rank is equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dead-ends and Spider Traps\n",
    "\n",
    "The main problems of simplified pagerank are dead-end nodes and spider traps structures.\n",
    "\n",
    "### 2.1 Dead-ends\n",
    "\n",
    "Dead-end is node with no outgoing edges. Since page importance has nowhere to be transferred, such nodes cause rank leakage.\n",
    "\n",
    "![2.png](2.png)\n",
    "\n",
    "Importance of node 2 has nowhere to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = toy_graph()\n",
    "G[2].clear() # By removing edges of node 2, we create a dead-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4282767441682065e-15, 4.996463459841385e-15, 4.996463459841385e-15, 4.996463459841385e-15]\n",
      "1.841766712369236e-14\n"
     ]
    }
   ],
   "source": [
    "rank_lst = pagerank(G)\n",
    "print(rank_lst)\n",
    "print(sum(rank_lst)) # Total rank is less than 1 due to rank leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spider Traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = toy_graph()\n",
    "G[2] = [2] # Node 2 absorbes all importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4282767441682065e-15, 4.996463459841385e-15, 0.9999999999999868, 4.996463459841385e-15]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "rank_lst = pagerank(G)\n",
    "print(rank_lst)\n",
    "print(sum(rank_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = toy_graph()\n",
    "G[2] = [4]\n",
    "G[4] = [2] # Node 2 and 4 absorb all importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.742621395334566e-15, 3.997170767873109e-15, 0.4499999999999953, 3.997170767873109e-15, 0.5499999999999937]\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "rank_lst = pagerank(G)\n",
    "print(rank_lst)\n",
    "print(sum(rank_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pagerank with Random Teleportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, beta=0.85, iteration_count=100):\n",
    "    \n",
    "    N = len(G.keys())\n",
    "    next_rank_lst = [1/N for _ in range(N)]\n",
    "    current_rank_lst = next_rank_lst[:]\n",
    "    \n",
    "    for i in range(iteration_count):\n",
    "        current_rank_lst, next_rank_lst = next_rank_lst, current_rank_lst\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] = (1 - beta) / N\n",
    "        for node in G:\n",
    "            if G[node]:\n",
    "                contribution = beta * (current_rank_lst[node] / len(G[node]))\n",
    "                for edge in G[node]:\n",
    "                    next_rank_lst[edge] += contribution\n",
    "        \n",
    "        leakage_contribution = (1 - sum(next_rank_lst)) / N\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] += leakage_contribution\n",
    "    \n",
    "    return next_rank_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = toy_graph()\n",
    "G[2].clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20833333333333334, 0.2638888888888889, 0.2638888888888889, 0.2638888888888889]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "rank_lst = pagerank(G, beta=0.8)\n",
    "print(rank_lst)\n",
    "print(sum(rank_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = toy_graph()\n",
    "G[2] = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10135135135135134, 0.12837837837837837, 0.6418918918918919, 0.12837837837837837]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "rank_lst = pagerank(G, beta=0.8)\n",
    "print(rank_lst)\n",
    "print(sum(rank_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spam Farms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = toy_graph()\n",
    "G[2].clear()\n",
    "for i in range(len(G.keys()), 100):\n",
    "    G[i] = [2]\n",
    "    G[2].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004054054054054079, 0.005135135135135165, 0.4409309308444677, 0.005135135135135165]\n"
     ]
    }
   ],
   "source": [
    "rank_lst = pagerank(G, beta=0.8)\n",
    "print(rank_lst[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TrustRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, beta=0.85, iteration_count=100, teleport_lst=None):\n",
    "    \n",
    "    if not teleport_lst:\n",
    "        teleport_lst = G.keys()\n",
    "    \n",
    "    N = len(G.keys())\n",
    "    next_rank_lst = [1/N for _ in range(N)]\n",
    "    current_rank_lst = next_rank_lst[:]\n",
    "    \n",
    "    teleport_lst_count = len(teleport_lst)\n",
    "    \n",
    "    for i in range(iteration_count):\n",
    "        current_rank_lst, next_rank_lst = next_rank_lst, current_rank_lst\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] = 0\n",
    "        for node in teleport_lst:\n",
    "            next_rank_lst[node] = (1 - beta) / teleport_lst_count\n",
    "        for node in G:\n",
    "            if G[node]:\n",
    "                contribution = beta * (current_rank_lst[node] / len(G[node]))\n",
    "                for edge in G[node]:\n",
    "                    next_rank_lst[edge] += contribution\n",
    "        \n",
    "        leakage_contribution = (1 - sum(next_rank_lst)) / N\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] += leakage_contribution\n",
    "        \n",
    "    return next_rank_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11583011583011586, 0.28957528957528955, 0.2488202487173264, 0.14671814671814676]\n"
     ]
    }
   ],
   "source": [
    "G = toy_graph()\n",
    "G[2].clear()\n",
    "for i in range(len(G.keys()), 100):\n",
    "    G[i] = [2]\n",
    "    G[2].append(i)\n",
    "\n",
    "trust_lst = [1]\n",
    "rank_lst = pagerank(G, beta=0.8, teleport_lst=trust_lst)\n",
    "print(rank_lst[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply PageRank on a Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "url = 'https://snap.stanford.edu/data/web-NotreDame.txt.gz'\n",
    "\n",
    "filename = 'data.txt'\n",
    "gzip_file = '%s.gz' % filename\n",
    "\n",
    "# Download the dataset\n",
    "is_download_required = not os.path.isfile(filename) \n",
    "\n",
    "if is_download_required:\n",
    "    urllib.request.urlretrieve(url, gzip_file)\n",
    "    \n",
    "    with gzip.open(gzip_file, 'rb') as f_in:\n",
    "        with open(filename, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse input file\n",
    "G = dict()\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        \n",
    "        u, v = map(int, line.split())\n",
    "        \n",
    "        if u not in G:\n",
    "            G[u] = list()\n",
    "        \n",
    "        G[u].append(v)\n",
    "\n",
    "N = max(G.keys()) + 1\n",
    "for i in range(N):\n",
    "    if i not in G:\n",
    "        G[i] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_lst = pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0054665478909549335,\n",
       " 0.00047967433074845605,\n",
       " 0.0002750863819726011,\n",
       " 0.0003679031647468208,\n",
       " 0.0003595245611761849,\n",
       " 0.000304996125513151,\n",
       " 0.0002926141797057847,\n",
       " 0.00030023960928669726,\n",
       " 0.0002836810879649729,\n",
       " 0.0002861092013107728]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_lst[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. One More Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, beta=0.85, iteration_count=100, teleport_lst=None, eps=1e-8):\n",
    "    \n",
    "    if not teleport_lst:\n",
    "        teleport_lst = G.keys()\n",
    "    \n",
    "    N = len(G.keys())\n",
    "    next_rank_lst = [1/N for _ in range(N)]\n",
    "    current_rank_lst = next_rank_lst[:]\n",
    "    \n",
    "    teleport_lst_count = len(teleport_lst)\n",
    "    \n",
    "    for i in range(iteration_count):\n",
    "        current_rank_lst, next_rank_lst = next_rank_lst, current_rank_lst\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] = 0\n",
    "        for node in teleport_lst:\n",
    "            next_rank_lst[node] = (1 - beta) / teleport_lst_count\n",
    "        for node in G:\n",
    "            if G[node]:\n",
    "                contribution = beta * (current_rank_lst[node] / len(G[node]))\n",
    "                for edge in G[node]:\n",
    "                    next_rank_lst[edge] += contribution\n",
    "        \n",
    "        leakage_contribution = (1 - sum(next_rank_lst)) / N\n",
    "        for j in range(N):\n",
    "            next_rank_lst[j] += leakage_contribution\n",
    "        \n",
    "        total_diff = 0\n",
    "        for c, n in zip(current_rank_lst, next_rank_lst):\n",
    "            total_diff += abs(c - n)\n",
    "        \n",
    "        if total_diff < eps:\n",
    "            return next_rank_lst\n",
    "    \n",
    "    return next_rank_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_lst = pagerank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " - Rajaraman, Anand, and Jeffrey David Ullman. Mining of massive datasets. Cambridge University Press, 2011.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
